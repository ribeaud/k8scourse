# Kubernetes: Hands-On Training Container Orchestration
Fabian Thorns <fabian.thorns@xamira.de>

June 19th to June 21st, 2023 -- Karakun AG


## Backlog

- If we have some time: Cluster setup
* Local development environments (Minikube? Karakun Dev Environments)
	Minikube
	https://k3s.io/ (only run on linux)

+ how to manage self-made images regarding security update (Erik)

+ Resource Limits / Quota (just a few questions) - Michael

## Resources


### Zoom Meeting

https://karakun.zoom.us/j/83256256497?from=addon
Meeting ID: 832 5625 6497
Passcode: 105901


### Handout

https://nextcloud.xamira.de/s/kLaBqfFJD33Zkcm


### VMs

xam-kube-01-1   49.13.29.53       2a01:4f8:c010:9ead::1 Fabian
xam-kube-01-2   49.12.242.155     2a01:4f8:c012:c4c3::1 Fabian
xam-kube-01-3   49.13.28.227      2a01:4f8:c012:f435::1 Fabian

xam-kube-03-1   49.13.48.66       2a01:4f8:c17:5fd3::1  Amit
xam-kube-03-2   168.119.157.218   2a01:4f8:c012:77de::1 Amit
xam-kube-03-3   167.235.232.19    2a01:4f8:c012:10bd::1 Amit

xam-kube-04-1   167.235.248.117   2a01:4f8:1c17:60c1::1 Fabio
xam-kube-04-2   128.140.48.128    2a01:4f8:c012:d08f::1 Fabio
xam-kube-04-3   49.13.6.106       2a01:4f8:c010:9bf6::1 Fabio

xam-kube-05-1   49.13.27.49       2a01:4f8:c012:d6d::1  Janak
xam-kube-05-2   138.201.119.176   2a01:4f8:c012:def::1  Janak
xam-kube-05-3   128.140.49.107    2a01:4f8:c17:620f::1  Janak

xam-kube-06-1   142.132.227.117   2a01:4f8:c010:9580::1 Jatin
xam-kube-06-2   159.69.51.99      2a01:4f8:c17:e0d9::1  Jatin
xam-kube-06-3   167.235.225.156   2a01:4f8:c012:ce3::1  Jatin

xam-kube-07-1   5.75.251.147      2a01:4f8:c012:a0b0::1 Marcel
xam-kube-07-2   167.235.255.199   2a01:4f8:c012:c4e::1  Marcel
xam-kube-07-3   167.235.194.232   2a01:4f8:c012:6ef::1  Marcel

xam-kube-08-1   116.202.13.16     2a01:4f8:c012:539b::1 Michael
xam-kube-08-2   167.235.192.6     2a01:4f8:c010:8551::1 Michael
xam-kube-08-3   167.235.252.54    2a01:4f8:c012:5a72::1 Michael

xam-kube-09-1   128.140.63.202    2a01:4f8:c012:10af::1 Olmo
xam-kube-09-2   5.75.245.223      2a01:4f8:c012:5b21::1 Olmo
xam-kube-09-3   49.13.19.16       2a01:4f8:c012:3b5a::1 Olmo

xam-kube-10-1   162.55.163.4      2a01:4f8:c012:af5e::1 Joseph
xam-kube-10-2   49.13.30.191      2a01:4f8:c012:46d9::1 Joseph
xam-kube-10-3   49.13.31.190      2a01:4f8:c012:7a02::1 Joseph

xam-kube-11-1   49.13.6.208       2a01:4f8:c17:9b0f::1  
xam-kube-11-2   49.13.7.35        2a01:4f8:c012:ea0b::1 
xam-kube-11-3   91.107.210.229    2a01:4f8:c012:76e::1  

xam-kube-12-1   142.132.239.214   2a01:4f8:c012:538d::1 
xam-kube-12-2   49.13.51.179      2a01:4f8:1c17:4283::1 
xam-kube-12-3   49.12.101.57      2a01:4f8:c012:679a::1 

xam-kube-13-1   49.13.28.126      2a01:4f8:c17:c7c9::1  Hannes
xam-kube-13-2   91.107.206.95     2a01:4f8:c012:ba9c::1 Hannes
xam-kube-13-3   5.75.230.70       2a01:4f8:c012:b24e::1 Hannes

xam-kube-14-1   49.12.244.178     2a01:4f8:c17:29c8::1  
xam-kube-14-2   49.13.7.116       2a01:4f8:c012:94dc::1 
xam-kube-14-3   78.46.225.234     2a01:4f8:c012:cbf5::1 

xam-kube-15-1   49.13.13.251      2a01:4f8:c012:3b9e::1 Daniel
xam-kube-15-2   157.90.240.174    2a01:4f8:c010:a815::1 Daniel
xam-kube-15-3   49.13.29.174      2a01:4f8:1c17:799e::1 Daniel

xam-kube-16-1   49.13.29.122      2a01:4f8:c012:2ff3::1 Erik
xam-kube-16-2   162.55.42.143     2a01:4f8:c17:acfa::1  Erik
xam-kube-16-3   78.46.218.228     2a01:4f8:c012:a4::1   Erik

xam-kube-17-1   116.202.29.154    2a01:4f8:c012:776e::1 Christian
xam-kube-17-2   168.119.186.159   2a01:4f8:c012:ac7f::1 Christian
xam-kube-17-3   142.132.186.101   2a01:4f8:c012:ca2::1  Christian

xam-kube-18-1   128.140.93.169    2a01:4f8:c012:73e6::1 
xam-kube-18-2   49.13.50.163      2a01:4f8:1c17:53f0::1 
xam-kube-18-3   49.13.18.229      2a01:4f8:c012:fcde::1 




## Kubernetes Architecture


* Control Plane Nodes ("Master Nodes")
++ Everything that is on a Worker Node
** API-Server
** etcd
** controller manager
** scheduler
** [cloud control manager]

* Worker Nodes ("Minions")
** Container Runtime (containerd, CRI-O, Docker Shim->Docker->containerd)
** kubelet
** kube-proxy

* Service
** CoreDNS
** Ingress Controller
** PKI
** Storage Provisioner
** Metrics server
** [Monitoring, Prometheus]
** [Logging, ELK, EFK, Loki, greylog2]



### EXERCISE: Setting up the work environment

* Check whether or not you already have a file ~/.kube/config. If this file exist create a backup.
* Open your backup in a text editor and make sure that is valid.
* Check that you can SSH into all of your virtual training machines.
* Copy the file /etc/kubernetes/admin.conf from the first machine to your local file ~/.kube/config .
* Make sure you have kubectl installed.
* The command `kubectl get nodes` on your local machine should now list your Kubernetes nodes.
* Install a text editor / IDE. If unsure, use Visual Studio Code, but everything you can handle is fine.
* If you use VS Code:
  * Install the Kubernetes extensions.
  * Open Settings, Extensions, Kubernetes, Edit in settings.json, add add these lines to the "vs-kubernetes" element
  "disable-linters": [
    "resource-limits"
  ]
* Create a directory `k8scourse` and open it in your IDE.

https://plugins.jetbrains.com/plugin/10485-kubernetes

Configure paths to kubectl and helm and check that the commands work:

https://www.jetbrains.com/help/idea/kubernetes.html




mkdir ~/.kube/
scp root@142.132.227.117:~/.kube/config  ~/.kube/config



## First Pod

apiVersion: v1
kind: Pod
metadata:
  name: website
spec:
  containers:
    - name: webserver
      image: httpd:latest

kubectl apply -f website-pod.yml
kubectl get pods
kubectl get pods -w
kubectl get pods -o wide
kubectl describe pod/website


### EXERCISE: Chasing down a Pod

* Start a first Pod which runs a container with the image httpd:latest.
* Find out, on which Node the Pod runs.
* SSH to that Node. Use `docker ps` to find which container(s) run(s) related to your Pod
* Use `docker inspect` to investigate the container's networking configuration. Does it look good?
* Kill the webserver container using `docker kill`. What happens in Docker now? Is it reflected in `kubectl get`and `kubectl describe`?
* If there are other containers related to the Pod, what happens if they are killed?



docker run alpine:latest /bin/sh
docker run -ti alpine:latest /bin/sh

kubectl exec -ti website -c webserver -- /bin/sh
kubectl logs website -c webserver -f


### EXERCISE: Interactive Pods

* Try to extend your current Pod with a second container running alpine:latest.
* Existing Pods may not be updated, use `kubectl delete` to remove the Pod before recreating it.
* If there are any issues with the Pod, try to figure out the reason using `kubectl describe`.
* Think about potential `docker run` parameters that may be necessary to prevent the same situation when using Docker.
* Use `kubectl explain` to find a similar option to use when defining containers in a Pod.
* When both containers run, use `kubectl exec` to open a shell (/bin/sh) in the Alpine pod and use `wget` or `curl` to query the web server.

apk add curl



apiVersion: v1
kind: Pod
metadata:
  name: website
spec:
  containers:
    - name: webserver
      image: httpd:latest
    - name: shell1
      image: alpine:latest
      stdin: true
      tty: true
    - name: shell2
      image: alpine:latest
      command:
        - /bin/sh
        - -c
      args:
        - while true; do date; sleep 1; done

kubectl exec -ti website -c shell1 -- /bin/sh
# wget -O - -q localhost

kubectl logs website -c shell2 -f

kubectl delete pod/website --grace-period=0



#### EXERCISE: Web-Clock in a single Pod

* Create one Pod with three containers:
  * clock:
    Runs an infinite loop to write the current time to a file every second. The file should reside on an emptyDir volume within the Pod.
  * timeserver:
    Runs httpd:latest. The emptyDir volume with the time file should be mounted to /usr/local/apache2/htdocs/.
  * client:
    Runs an infitite loop that retrieves the time file from the webserver every second and prints it to standard out (e.g. using `curl -s` or `wget -O - -q`)
* When the Pod runs, `kubectl logs` on the client should output the current time every second.



apiVersion: v1
kind: Pod
metadata:
  name: k8stime
spec:
  containers:
    - name: webserver
      image: httpd:latest
      volumeMounts:
        - name: htdocs
          mountPath: /usr/local/apache2/htdocs
    - name: clock
      image: alpine:latest
      command:
        - /bin/sh
        - -c
      args:
        - while true; do date > /mnt/date.txt; sleep 1; done
      volumeMounts:
        - name: htdocs
          mountPath: /mnt
    - name: client
      image: alpine:latest
      command:
        - /bin/sh
        - -c
      args:
        - while true; do wget -O - -q http://localhost/date.txt; sleep 1; done
  volumes:
    - name: htdocs
      emptyDir: {}



## PersistentVolumeClaims

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc3
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi


#### EXERCISE: Web-Clock with a PVC

* Continue working on the previous example.
* Create a PVC to store the time data.
* Create a new Pod that only contains the clock, but not the timeserver and the client. The Pod should mount the PVC instead of the emptyDir volume.
* Remove the clock from the Pod of the previous exercise. Change the emptyDir volume to also mount the PVC.
* To allow both Pods to access the volume, make sure they both run on the same node. Specify the name of one of the worker nodes in the Pod.spec.nodeName of the Pods.


#### Solutions

  clock-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: clock
spec:
  nodeName: xam-kube-01-2
  containers:
    - name: clock
      image: alpine:latest
      command:
        - /bin/sh
        - -c
      args:
        - while true; do date > /mnt/date.txt; sleep 1; done
      volumeMounts:
        - name: htdocs
          mountPath: /mnt
  volumes:
    - name: htdocs
      persistentVolumeClaim:
        claimName: timedata

  k8stime-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: k8stime
spec:
  nodeName: xam-kube-01-2
  containers:
    - name: webserver
      image: httpd:latest
      volumeMounts:
        - name: htdocs
          mountPath: /usr/local/apache2/htdocs
    - name: client
      image: alpine:latest
      command:
        - /bin/sh
        - -c
      args:
        - while true; do wget -O - -q http://localhost/date.txt; sleep 1; done
  volumes:
    - name: htdocs
      persistentVolumeClaim:
        claimName: timedata

  timedata-pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: timedata
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  

## Labels

https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/

kubectl get pods --show-labels -l foo=bar


#### EXERCISE: Getting started with Labels

Run the following set of pods:

kubectl run web1 --restart=Never --image=httpd --labels app=aapp,component=c1,version=1.0
kubectl run web2 --restart=Never --image=httpd --labels app=aapp,component=c1,version=1.0
kubectl run web3 --restart=Never --image=httpd --labels app=aapp,component=c2,version=1.1
kubectl run web4 --restart=Never --image=httpd --labels app=aapp,component=c3,version=1.0
kubectl run web5 --restart=Never --image=httpd --labels app=bapp,component=c1,version=1.1
kubectl run web6 --restart=Never --image=httpd --labels app=bapp,component=c2,version=1.0
kubectl run web7 --restart=Never --image=httpd --labels app=bapp,component=c3,version=1.1
kubectl run web8 --restart=Never --image=httpd --labels app=bapp,component=c4,version=1.1

Try to create kubectl get commands that return these Pods:

* All Pods with the version 1.1.
kubectl get pods --show-labels -l "version=1.1"

* All Pods with the component c1.
kubectl get pods --show-labels -l "component=c1"

* All Pods with either the component c1 or c2.
kubectl get pods --show-labels -l "component in (c1,c2)"

* All Pods which have a different component than c2, with that component being in version 1.1.version=1.1
kubectl get pods --show-labels -l "component notin (c2),version=1.1"
kubectl get pods --show-labels -l "component!=c2,version=1.1"

* Use the command `kubectl label --overwrite` to change the labels of all Pods with a component c2 to version 1.2
kubectl label pod --overwrite version=1.2 -l component=c2

* Use the command `kubectl label --overwrite` to change the labels of all Pods with a component c3 or c4, both in version 1.1, to version 1.3
kubectl label pod --overwrite version=1.3 -l "component in (c3,c4),version=1.1"

Add --dry-run=client for a dry run.
kubectl run web --image=httpd:latest --dry-run=client -o yaml


## Services

apiVersion: v1
kind: Service
metadata:
  name: aapp-c1
spec:
  type: ClusterIP
  ports:
    - port: 80
      protocol: TCP
      targetPort: 80
  selector:
    app: aapp
    component: c1


#### EXERCISE: Web-Clock with a Service

* Continue working on the previous example.
* Create a new Pod that only contains the timeserver. Add labels to the Pod to specifically search for this Pod.
* Create a Service that selects the timeserver pod. Check that the Endpoints of the service match the Pod's IP address.
* Remove the timeserver from the previous example's Pod so it only contains the client. Adjust the client command to query the Service instead of localhost.
* Ensure all resources of this exercise have proper labels.
* Run all resources


client-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: client
  labels:
    apps: k8stime
    component: client
spec:
  containers:
    - name: client
      image: alpine:latest
      command:
        - /bin/sh
        - -c
      args:
        - while true; do wget -O - -q http://timeserver/date.txt; sleep 1; done

clock-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: clock
  labels:
    apps: k8stime
    component: clock
spec:
  nodeName: xam-kube-01-3
  containers:
    - name: clock
      image: alpine:latest
      command:
        - /bin/sh
        - -c
      args:
        - while true; do date > /mnt/date.txt; sleep 1; done
      volumeMounts:
        - name: htdocs
          mountPath: /mnt
  volumes:
    - name: htdocs
      persistentVolumeClaim:
        claimName: timedata

timedata-pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: timedata
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  
timeserver-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: timeserver
  labels:
    apps: k8stime
    component: timeserver
spec:
  nodeName: xam-kube-01-3
  containers:
    - name: webserver
      image: httpd:latest
      volumeMounts:
        - name: htdocs
          mountPath: /usr/local/apache2/htdocs
  volumes:
    - name: htdocs
      persistentVolumeClaim:
        claimName: timedata

timeserver-svc.yml
apiVersion: v1
kind: Service
metadata:
  name: timeserver
spec:
  type: ClusterIP
  ports:
    - port: 80
      protocol: TCP
      targetPort: 80
  selector:
    apps: k8stime
    component: timeserver


Access some port within kubernetes locally:
   kubectl port-forward svc/timeserver 8080:80



## Probes

In Pod.spec.containers:
      readinessProbe:
        httpGet:
          port: 80
          path: /ready.txt

#### EXERCISE: Web-clock with Probes

* Add readinessProbe to the timeserver. The probe should be an httpGet probe to `/ready.txt`.
* Is the probe successful? Can this be seen in `kubectl get` and `kubectl describe`?
* Is the Pod included as an Endpoint in the Service? (`kubectl get ep`)
* Use kubectl exec to enter the timeserver Pod and create the file `/usr/local/apache2/htdocs/ready.txt`.
  kubectl exec -ti pod/timeserver -- /bin/bash
* What happens when the probe becomes true? Is it included in the Service Endpoints?

* Add a livenessProbe to the timeserver. The probe should be an httpGet probe to `/alive.txt`.
* What happens if the livenessProbe fails?
* Make sure the Pod passes the livenessProbe.

* Keep the logs of the timeserver open to monitor what files the kubelet requests (kubectl logs <pod-name> -c <container-name> -f).

* Add a startupProbe to the timeserver. The probe should be an httpGet probe to `/started.txt`.
* What happens if the startupProbe fails?
* What happens if the startupProbe becomes successful?


1. Dockerfile
2. Build the Dockerfile -> Image
3. Push the image to a registry
4. Reference the image in a Pod



Docker image registry:latest

PVC: 2Gi
Pod: Image: registry:latest, must mount PVC at /var/lib/registry/
SVC: Port: 5000

root@xam-kube-17-1:~# cat /etc/docker/daemon.json 
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "insecure-registries" : [ "10.96.0.0/12", "127.0.0.0/8" ]
}

#### EXERCISE: Registry Setup

* Add a simple container registry to your Kubernetes.
* The registry should be provided by the image registry:latest.
* Find out which port the registry uses and under which path it stores image data.
* Define which Kubernetes resources are required to run the registry in a way that the container can be restarted without loosing image data and while keeping the registry accessible through a consistent IP address.
* Create the required Kubernetes resources.
On one of the k8s nodes:
* To see if the the registry runs, run `curl http://ip:port/v2/_catalog` on any of the Kubernetes Nodes. Adjust ip and port according to your setup. The result should be an empty `repositories` array.
* To test the registry, download an existing image using `docker pull alpine:latest`, add an additional tag to the image using `docker tag alpine:latest ip:port/alpine:latest` and upload the image to your registry using `docker push ip:port/alpine:latest`. Now the registry catalog should include alpine.



#### registry-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: registry
  labels:
    apps: registry
    component: registry
spec:
  containers:
    - name: registry
      image: registry:latest
      volumeMounts:
        - name: registry
          mountPath: /var/lib/registry/
  volumes:
    - name: registry
      persistentVolumeClaim:
        claimName: registry


#### registry-pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: registry
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi

#### svc-registry.yml
apiVersion: v1
kind: Service
metadata:
  name: registry
spec:
  type: ClusterIP
  ports:
    - port: 5000
      protocol: TCP
      targetPort: 5000
  selector:
    apps: registry
    component: registry


#### EXERCISE: Dockercoins

* Clone the container.training git repository on one of your nodes:
     git clone https://github.com/jpetazzo/container.training
* Within the directory container.training/dockercoins, build the images for rng, hasher, worker and webui. Push the images to your registry.
* Ensure that there is a container for rng, hasher, worker, webui and redis running.
* The redis container should have a persistent volume mounted to /data
* Create services for rng, hasher, redis and webui. Except for redis, all services run on Port 80. Redis runs on Port 6379.
* Open the webui in your browser and make sure you see some coin mining.

docker build -t ip:port/rng:latest rng/
docker push ip:port/rng:latest

docker build -t rng:latest rng/
docker tag rng:latest ip:port/rng:latest
docker push ip:port/rng:latest

To externally access `webui`, you have to use `NodePort` in the `webui` service. By combining the IP address of one of the cluster nodes, and the port number listed in `kubectl get service/webui` you are able to access `webui` in the browser. Alternatively one could use `kubectl port-forward`.

To access a service from outside (browser)
	* Create the service with type ‘NodePort’
	* Get the port of the service using ‘kubectl get svc’
	* Get the IP address of the node where POD is running using ‘kubectl get nodes -o wide’. Take IP address from the INTERNAL-IP column.




for i in hasher rng webui worker
do
 docker build -t 10.97.235.125:5000/$i:latest $i
 docker push 10.97.235.125:5000/$i:latest
done

 for i in worker hasher webui redis
> do
> cp rng-pod.yml $i-pod.yml
> cp rng-svc.yml $i-svc.yml
> sed -i -e "s/rng/$i/" $i-pod.yml $i-svc.yml
> done




hasher-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: hasher
  labels:
    apps: k8scoins
    component: hasher
spec:
  containers:
    - name: hasher
      image: 10.97.235.125:5000/hasher:latest
      ports:
        - name: http
          protocol: TCP
          containerPort: 80

hasher-svc.yml
apiVersion: v1
kind: Service
metadata:
  name: hasher
spec:
  type: ClusterIP
  ports:
    - port: 80
      protocol: TCP
      targetPort: http
  selector:
    apps: k8scoins
    component: hasher

redis-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: redis
  labels:
    apps: k8scoins
    component: redis
spec:
  containers:
    - name: redis
      image: redis:latest
      ports:
        - name: redis
          protocol: TCP
          containerPort: 6379
      volumeMounts:
        - name: redis
          mountPath: /data
  volumes:
    - name: redis
      persistentVolumeClaim:
        claimName: redis

redis-pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: redis
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

redis-svc.yml
apiVersion: v1
kind: Service
metadata:
  name: redis
spec:
  type: ClusterIP
  ports:
    - port: 6379
      protocol: TCP
      targetPort: redis
  selector:
    apps: k8scoins
    component: redis

rng-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: rng
  labels:
    apps: k8scoins
    component: rng
spec:
  containers:
    - name: rng
      image: 10.97.235.125:5000/rng:latest
      ports:
        - name: http
          protocol: TCP
          containerPort: 80

rng-svc.yml
apiVersion: v1
kind: Service
metadata:
  name: rng
spec:
  type: ClusterIP
  ports:
    - port: 80
      protocol: TCP
      targetPort: http
  selector:
    apps: k8scoins
    component: rng

webui-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: webui
  labels:
    apps: k8scoins
    component: webui
spec:
  containers:
    - name: webui
      image: 10.97.235.125:5000/webui:latest
      ports:
        - name: http
          protocol: TCP
          containerPort: 80

webui-svc.yml
apiVersion: v1
kind: Service
metadata:
  name: webui
spec:
  type: NodePort
  ports:
    - port: 80
      protocol: TCP
      targetPort: http
      nodePort: 31234
  selector:
    apps: k8scoins
    component: webui

worker-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: worker
  labels:
    apps: k8scoins
    component: worker
spec:
  containers:
    - name: worker
      image: 10.97.235.125:5000/worker:latest
      ports:
        - name: http
          protocol: TCP
          containerPort: 80

## Deployment

apiVersion: apps/v1
kind: Deployment
metadata:
  name: webui
  labels:
    apps: k8scoins
    component: webui
  annotations:
    kubernetes.io/change-cause: "Update to summer edition"
spec:
  replicas: 1
  selector:
    matchLabels:
      apps: k8scoins
      component: webui
  template:
    metadata:
      labels:
        apps: k8scoins
        component: webui
    spec:
      containers:
        - name: webui
          image: 10.97.235.125:5000/webui:latest
          ports:
            - name: http
              protocol: TCP
              containerPort: 80

#### EXERCISE: Dockercoins with Deployments

* Turn all five Pods into Deployments
* Delete the currently running, manually started Pods
* Run the Deployments and confirm that Deployments, ReplicaSets and Pods exist as expected


watch -n 1 kubectl get deploy,rs,pods (use Homebrew for macosx)


#### EXERCISE: Scaling and Rolling Updates

* Scale the webui to 10. Do this by editing the yaml file and applying it.
* Wait for all 10 Pods to be running and ready.
* Open `watch -n 1 kubectl get deploy,rs,pod` in a terminal and keep it running all the time
* Add a readiness probe (httpGet to /ready.txt) to the webui Deployment template. Don't change anything else.
* Apply the updated yaml file.
* Observe what happens. Wait till no more changes happen.
* Make one of the new Pods ready (kubectl exec ... -- touch /files/ready.txt). What happens now?


## DaemonSet

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: rng
  labels:
    apps: k8scoins
    component: rng
  annotations:
    kubernetes.io/change-cause: "Update to summer edition"
spec:
  selector:
    matchLabels:
      apps: k8scoins
      component: rng
  template:
    metadata:
      labels:
        apps: k8scoins
        component: rng
    spec:
      containers:
        - name: rng
          image: 10.97.235.125:5000/rng:latest
          ports:
            - name: http
              protocol: TCP
              containerPort: 80


#### EXERCISE: DaemonSet for RNG

* Turn your rng into a daemonset.
* Delete the rng Deployment.
* Keep `watch -n 1 kubectl get ds,pods` running all the time.
* Apply the DaemonSet
* How many pods are created and what nodes do they run on?

* Add this to the DaemonSet at spec.template.spec:
      nodeSelector:
        training: run-ds
* Apply the configuration.
* How many Pods run now? What can be done to change this?

* remove the nodeSelector from the DaemonSet and instead add this to pec.template.spec:
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          effect: NoSchedule
* Apply the configuration.
* How many Pods run now? Can you explain the observed behavior, i.e. when reviewing the output of `kubectl describe` for the various Nodes?

## StatefulSet

apiVersion: v1
kind: Service
metadata:
  name: training-stateful-web
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - port: 80
  selector:
    app: training
    component: stateful-web

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: training-stateful-web
  labels:
    app: training
    component: stateful-web
spec:
  replicas: 3
  serviceName: training-stateful-web
  selector:
    matchLabels:
      app: training
      component: stateful-web
  template:
    metadata:
      labels:
        app: training
        component: stateful-web
    spec:
      containers:
        - name: webserver
          image: nginx
          ports:
            - containerPort: 80
              name: web
          readinessProbe:
            httpGet:
              path: /ready.txt
              port: 80
          volumeMounts:
            - name: htdocs
              mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
    - metadata:
        name: htdocs
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi


## Job

apiVersion: batch/v1
kind: Job
metadata:
  name: training-count-once
spec:
  template:
    spec:
      containers:
        - name: training
          image: alpine:latest
          command:
            - "/bin/sh"
            - "-c"
            - "for i in $(seq 10); do echo $(date): $i; sleep 1; done"
      restartPolicy: Never

## CronJob


apiVersion: batch/v1
kind: CronJob
metadata:
  name: training-count-every-five-minutes
spec:
  schedule: "*/5 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: training
              image: alpine:latest
              command:
                - "/bin/sh"
                - "-c"
                - "for i in $(seq 10); do echo $(date): $i; sleep 1; done"
          restartPolicy: OnFailure


## Ingress

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rng
spec:
  ingressClassName: nginx
  rules:
    - host: rng.49.12.242.155.nip.io
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
	              service:
                name: rng
                port:
                  number: 80

kubectl get nodes -o wide
-> Get Node ip -> a.b.c.d
Create a hostname like this: rng.a.b.c.d.nip.io

* Create an Ingress for the webui and test it
* Create a second Ingress for the rng



#### EXERCISE: Resource Consumption

* Run a Pod on Kubernets. Identify the Node the Pod runs on and SSH into the node. Open `top` and keep it open.
* Exec into the Pod and install stress / stress-ng in the Pod.
* Run `stress -c 4` or `stress-ng -c 4` and see how many resource the Pod can consume.

Alpine: apk add stress-ng
Ubuntu: apt-get update && apt-get install -y stress

kubectl run stress --image alpine:latest
kubectl run stress2 --image=alpine:latest -ti  

kubectl exec stress -ti -- apk add stress-ng
kubectl exec stress -ti -- stress-ng -c 4


apiVersion: v1
kind: Pod
metadata:
  name: stress
spec:
  containers:
    - name: stress
      image: 10.97.235.125:5000/alpine:latest
      tty: true
      stdin: true
      resources:
        requests:
          cpu: 100m
          memory: 64Mi
        limits:
          cpu: 500m
          memory: 128Mi


kubectl create ns client1time
kubectl apply -f k8stime -n client1time



apiVersion: v1
kind: LimitRange
metadata:
  name: training-namespace-low-limits
spec:
  limits:
    - type: Container
      defaultRequest:
        cpu: 100m
        memory: 64Mi
      default:
        cpu: 200m
        memory: 128Mi
      min:
        cpu: 100m
        memory: 16Mi
      max:
        cpu: 250m
        memory: 256Mi
    - type: Pod
      min:
        cpu: 100m
        memory: 16Mi
      max:
        cpu: "0.5"
        memory: 512Mi

apiVersion: v1
kind: ResourceQuota
metadata:
  name: training-namespace-small-quota
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
    count/pods: "4"
    count/persistentvolumeclaims: "2"
    count/services: "2"
    count/services.nodeports: "1"

## Helm

helm create helmcoins
cd helmcoins
echo > values.yaml
rm -r templates/*

cp ../k8scoins/* templates/
rm template/ing-*
cd ..
helm package helmcoins

kubectl create ns helmcoins
helm install -n helmcoins fabiancoins ./helmcoins-0.1.1.tgz
helm upgrade -n helmcoins fabiancoins ./helmcoins-0.1.1.tgz
helm list -n helmcoins


values.yaml:
ingress:
  domain: 49.12.242.155.nip.io

In the template:
        - host: webui.{{ .Values.ingress.domain }}

Install:
    helm upgrade -n helmcoins fabiancoins ./helmcoins-0.1.4.tgz --set ingress.domain=helmcoins.49.12.242.155.nip.io

Alternatively:
client5-values.yml
ingress:
  domain: client5.49.12.242.155.nip.io
helm install -n client5 client5-install ./helmcoins-0.1.4.tgz -f client5-values.yml

{{- if .Values.ingress.enabled -}}
{{- end }}


* Add the Ingress to the chart
* Configure the hostname from a values variable
* Make the ingress optional
* Install, just by having specific values files:
    client1coins: Ingress with client1 in domain name
    client2coins: Ingress with client2 in domain name
    client3coins: Ingress without an ingress

https://helm.sh/docs/chart_template_guide/builtin_objects/

helm install --create-namespace -n foo ....


apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: HelmRepository
metadata:
  name: apps
spec:
  url: https://<host>/<org>/charts
  interval: 1m


apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: app
spec:
  chart:
    spec:
      chart: app # name oder pfad
      version: "4.0.x" # kann auch ein range sein >=4.0.0 <5.0.0) , funktioniert nur für HelmRepository
      sourceRef:
        kind: HelmRepository # oder GitRepository oder Bucket
        name: apps
#       namespace: ...
#     interval: 1m
  values:
    replicas: 2
  valuesFrom: # werden im Zweifelsfall von values überschrieben!
    - kind: ConfigMap
      name: bla
      valuesKey: dbname
#      targetPath: db.conf.name



## Thank you so much for your course participation!

Feel free to connect:

 - https://www.linkedin.com/in/fthorns/
 - https://www.xing.com/profile/Fabian_Thorns/

 - https://www.linkedin.com/company/xamira-networks-gmbh/
 - https://www.xing.com/pages/xamira-networks-gmbh

We would be happy if you give us feedback (please click through all pages, even if you don't enter any information there -- only if you submit the questionnaire at the end, the entries will be saved):

  https://forms.gle/gwff43qVexpzqCcC7

You can find more information about our course program at www.xamira.de. We would be happy to welcome you again in one of our trainings!





